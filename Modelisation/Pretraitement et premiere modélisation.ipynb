{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719a29cd",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b826dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import pickle\n",
    "\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "import pickle\n",
    "import plotly.offline as pyo\n",
    "import plotly.express as px\n",
    "# Set notebook mode to work in offline\n",
    "pyo.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097dedb",
   "metadata": {},
   "source": [
    "# Fonction d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56bcbbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_alpha_score(y_true, y_pred):\n",
    "    alpha = 0.1\n",
    "    \n",
    "    \n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()    \n",
    "        \n",
    "    f_alpha_score = ( 2 * tp) / (2 * tp + alpha *fp +  fn)\n",
    "    f_alpha_score = float(\"{:.5f}\".format(f_alpha_score)) \n",
    "    \n",
    "    return f_alpha_score\n",
    "\n",
    "# pour LGBM\n",
    "def F_alpha_score_lbgm(y_true, y_pred, printScore = False):\n",
    "    f_alpha_score = F_alpha_score(y_true, y_pred)\n",
    "    is_higher_better = True\n",
    "    return \"f10score\", f_alpha_score, is_higher_better \n",
    "\n",
    "def F_alpha_score_mlflow(eval_df, _builtin_metrics):\n",
    "    return F_alpha_score(eval_df[\"target\"], eval_df[\"prediction\"])\n",
    "        \n",
    "def F_alpha_score_bayes_search(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    f_alpha_score = F_alpha_score(y, y_pred)\n",
    "    return f_alpha_score\n",
    "    \n",
    "\n",
    "def eval_metrics(y_test, y_pred):\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    f10_score = F_alpha_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return recall, precision, f1_score, accuracy, f10_score, auc\n",
    "\n",
    "# def conf_mat_transform(y_true, y_pred):\n",
    "#     conf_mat = metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#     labels = pd.Series(y_true, name=\"y_true\").to_frame()\n",
    "#     labels['y_pred'] = y_pred\n",
    "#     labels['y_pred_transform'] = labels['y_pred'].apply(lambda x: corresp[x])\n",
    "\n",
    "#     return labels['y_pred_transform']\n",
    "\n",
    "\n",
    "# def DisplayConfMatrix(conf_mat, display_labels):\n",
    "#     cm_display = metrics.ConfusionMatrixDisplay(\n",
    "#         confusion_matrix=conf_mat, display_labels=display_labels)\n",
    "#     fig, ax = plt.subplots(figsize=(15, 10))\n",
    "#     cm_display.plot(ax=ax)\n",
    "#     ax.tick_params(axis='x', labelrotation=45)\n",
    "#     plt.show()\n",
    "    \n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360f21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MlFlow_log_score(modele, nom_du_modele, run_name, X_test, y_test):\n",
    "\n",
    "    with mlflow.start_run(run_name= run_name):\n",
    "\n",
    "        signature = infer_signature(X_test, y_test)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            modele, \"model\", registered_model_name=nom_du_modele, signature=signature)\n",
    "\n",
    "        model_uri = mlflow.get_artifact_uri(\"model\")\n",
    "\n",
    "        f_alpha_score_metric = mlflow.models.make_metric(\n",
    "            eval_fn=F_alpha_score_mlflow, greater_is_better=True)\n",
    "        eval_data = valid_x.copy()\n",
    "        eval_data[\"TARGET\"] = valid_y\n",
    "\n",
    "        # Evaluate the logged model\n",
    "        result = mlflow.evaluate(model_uri, eval_data, targets=\"TARGET\", model_type=\"classifier\", evaluators=[\n",
    "                                 \"default\"], custom_metrics=[f_alpha_score_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38fe7d",
   "metadata": {},
   "source": [
    "# Fonction de traitement  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec2cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14413e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess application_train.csv and application_test.csv\n",
    "def application_train_test(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv('input/application_train.csv', nrows= num_rows)\n",
    "#     test_df = pd.read_csv('input/application_test.csv', nrows= num_rows)\n",
    "#     print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "#     df = pd.concat([df, test_df], ignore_index=True)\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    # Some simple new features (percentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "#     del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef2d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv('input/bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv('input/bureau_balance.csv', nrows = num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf173242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    prev = pd.read_csv('input/previous_application.csv', nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb56a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    pos = pd.read_csv('input/POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf0e9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    ins = pd.read_csv('input/installments_payments.csv', nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a202e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(nan_as_category = True):\n",
    "    cc = pd.read_csv('input/credit_card_balance.csv')\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b1928e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 103558 entries, 100006 to 456250\n",
      "Columns: 141 entries, CC_MONTHS_BALANCE_MIN to CC_COUNT\n",
      "dtypes: bool(16), float64(101), int64(24)\n",
      "memory usage: 101.1 MB\n"
     ]
    }
   ],
   "source": [
    "a = credit_card_balance() \n",
    "a.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a261f",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c577d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, log_reg = True):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df[feats], train_df['TARGET'], train_size=0.8) \n",
    "    \n",
    "    print(\"Starting LightGBM. Train shape: {}, valid shape: {}\".format(train_x.shape, train_y.shape))\n",
    "    \n",
    "     \n",
    "    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df[feats], train_df['TARGET'], train_size=0.8) \n",
    "\n",
    "    # Dummy classifier\n",
    "    dum_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "    dum_clf.fit(train_x, train_y)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    if(log_reg):\n",
    "        log_clf.fit(train_x, train_y)\n",
    "    \n",
    "    \n",
    "    # LightGBM\n",
    "    lgbm_clf = LGBMClassifier(random_state=42)\n",
    "\n",
    "    lgbm_clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "        eval_metric= 'F_alpha_score_lbgm', verbose= 200, early_stopping_rounds= 200)\n",
    "    \n",
    "    \n",
    "    return dum_clf, log_clf, lgbm_clf, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98bde05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(df):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df[feats], train_df['TARGET'], train_size=0.8) \n",
    "\n",
    "    # LightGBM\n",
    "    lgbm_clf = LGBMClassifier(random_state=42)\n",
    "\n",
    "    lgbm_clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "        eval_metric= 'F_alpha_score_lbgm', verbose= 200, early_stopping_rounds= 200)\n",
    "    \n",
    "    \n",
    "    return lgbm_clf, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c96b9770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, num_folds, stratified = False, debug= False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    del df\n",
    "    gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    # Create arrays and dataframes to store results\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for  n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=5000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric= 'F_alpha_score_lbgm', verbose= 200, early_stopping_rounds= 200)\n",
    "       \n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de587cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (160870, 200), valid shape: (160870,)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's binary_logloss: 0.233317\tvalid_1's binary_logloss: 0.249975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Dummy classifier' already exists. Creating a new version of this model...\n",
      "2023/09/27 20:19:26 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: Dummy classifier, version 15\n",
      "Created version '15' of model 'Dummy classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a383ec65bdc345238e08507cb0696050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/27 20:19:27 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/09/27 20:19:27 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1, negative label is 0.\n",
      "2023/09/27 20:19:29 WARNING mlflow.models.evaluation.default_evaluator: Skip logging model explainability insights because the shap explainer None requires all feature values to be numeric, and each feature column must only contain scalar values.\n",
      "Registered model 'Log reg classifier' already exists. Creating a new version of this model...\n",
      "2023/09/27 20:19:32 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: Log reg classifier, version 15\n",
      "Created version '15' of model 'Log reg classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab5c3bf874a470fafa8415c413069e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/27 20:19:32 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/09/27 20:19:32 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1, negative label is 0.\n",
      "2023/09/27 20:19:35 WARNING mlflow.models.evaluation.default_evaluator: Skip logging model explainability insights because the shap explainer None requires all feature values to be numeric, and each feature column must only contain scalar values.\n",
      "Registered model 'LGBM classifier' already exists. Creating a new version of this model...\n",
      "2023/09/27 20:19:39 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: LGBM classifier, version 84\n",
      "Created version '84' of model 'LGBM classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a77cdfeb1eb43efba7b6f370f7335aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/27 20:19:39 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/09/27 20:19:39 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1, negative label is 0.\n",
      "2023/09/27 20:19:42 WARNING mlflow.models.evaluation.default_evaluator: Skip logging model explainability insights because the shap explainer None requires all feature values to be numeric, and each feature column must only contain scalar values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process application train and test - done in 60s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1050x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with timer(\"Process application train and test\"):\n",
    "    df = application_train_test()    \n",
    "    \n",
    "    # Suppression des valeurs null, afin de pouvoir lancer la log reg\n",
    "        # Colonnes\n",
    "    null_values = (df.isnull().sum() / len(df)) * 100\n",
    "    columns_to_delete = null_values[null_values > 30].index\n",
    "    df_clean = df.drop(columns=columns_to_delete)\n",
    "        # Lignes\n",
    "    df_clean.dropna(inplace=True)\n",
    "    \n",
    "    dum_clf, log_clf, lgbm_clf, valid_x, valid_y = train_models(df_clean)\n",
    "    \n",
    "    MlFlow_log_score(dum_clf, \"Dummy classifier\", \"dummy\", valid_x, valid_y)\n",
    "    \n",
    "    MlFlow_log_score(log_clf, \"Log reg classifier\", \"logistic regression\", valid_x, valid_y)\n",
    "    \n",
    "    MlFlow_log_score(lgbm_clf, \"LGBM classifier\", \"lgbm: application_train\", valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f806594",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's binary_logloss: 0.22553\tvalid_1's binary_logloss: 0.248972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LGBM classifier' already exists. Creating a new version of this model...\n",
      "2023/09/27 20:20:27 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: LGBM classifier, version 85\n",
      "Created version '85' of model 'LGBM classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0183c7eda6f435f8e3767e075c6f646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/27 20:20:28 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/09/27 20:20:29 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1, negative label is 0.\n",
      "2023/09/27 20:20:34 WARNING mlflow.models.evaluation.default_evaluator: Skip logging model explainability insights because the shap explainer None requires all feature values to be numeric, and each feature column must only contain scalar values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process bureau and bureau_balance - done in 52s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Process bureau and bureau_balance\"):\n",
    "    bureau = bureau_and_balance()\n",
    "               \n",
    "    df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del bureau\n",
    "    gc.collect()    \n",
    "        \n",
    "    lgbm_clf, valid_x, valid_y = train_lgbm(df)    \n",
    "    \n",
    "    MlFlow_log_score(lgbm_clf, \"LGBM classifier\", \"lgbm: add bureau and balance\", valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a8966fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's binary_logloss: 0.222144\tvalid_1's binary_logloss: 0.244543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LGBM classifier' already exists. Creating a new version of this model...\n",
      "2023/09/27 20:21:56 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: LGBM classifier, version 86\n",
      "Created version '86' of model 'LGBM classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6362198c9a146009e3a6dab941e4f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/27 20:21:57 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/09/27 20:22:00 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1, negative label is 0.\n",
      "2023/09/27 20:22:07 WARNING mlflow.models.evaluation.default_evaluator: Skip logging model explainability insights because the shap explainer None requires all feature values to be numeric, and each feature column must only contain scalar values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process previous_applications - done in 94s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Process previous_applications\"):\n",
    "    prev = previous_applications()\n",
    "\n",
    "    df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "    del prev\n",
    "    gc.collect()\n",
    "    \n",
    "    lgbm_clf, valid_x, valid_y = train_lgbm(df)    \n",
    "    \n",
    "    MlFlow_log_score(lgbm_clf, \"LGBM classifier\", \"lgbm: add previous applications\",  valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c93155c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos-cash balance df shape: (337252, 18)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's binary_logloss: 0.222404\tvalid_1's binary_logloss: 0.239369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LGBM classifier' already exists. Creating a new version of this model...\n",
      "2023/09/27 20:23:16 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: LGBM classifier, version 87\n",
      "Created version '87' of model 'LGBM classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2050250c504143b310ae42cd292b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/27 20:23:17 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/09/27 20:23:20 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1, negative label is 0.\n",
      "2023/09/27 20:23:28 WARNING mlflow.models.evaluation.default_evaluator: Skip logging model explainability insights because the shap explainer None requires all feature values to be numeric, and each feature column must only contain scalar values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process POS-CASH balance - done in 80s\n"
     ]
    }
   ],
   "source": [
    " with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash()\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    \n",
    "        lgbm_clf, valid_x, valid_y = train_lgbm(df)    \n",
    "\n",
    "        MlFlow_log_score(lgbm_clf, \"LGBM classifier\", \"lgbm: add POS-CASH balance\", valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a4f9778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's binary_logloss: 0.221953\tvalid_1's binary_logloss: 0.234052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LGBM classifier' already exists. Creating a new version of this model...\n",
      "2023/09/27 20:24:45 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: LGBM classifier, version 88\n",
      "Created version '88' of model 'LGBM classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2579484495c745e785980b2b2cc34a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/27 20:24:46 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/09/27 20:24:49 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1, negative label is 0.\n",
      "2023/09/27 20:24:57 WARNING mlflow.models.evaluation.default_evaluator: Skip logging model explainability insights because the shap explainer None requires all feature values to be numeric, and each feature column must only contain scalar values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process installments payments - done in 90s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments()\n",
    "        \n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    \n",
    "        lgbm_clf, valid_x, valid_y = train_lgbm(df)    \n",
    "\n",
    "        MlFlow_log_score(lgbm_clf, \"LGBM classifier\", \"lgbm: add installments payments\", valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95f588ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's binary_logloss: 0.220345\tvalid_1's binary_logloss: 0.237554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LGBM classifier' already exists. Creating a new version of this model...\n",
      "2023/09/27 20:26:30 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: LGBM classifier, version 89\n",
      "Created version '89' of model 'LGBM classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1cc575d0cf48cd848234e7aafd9b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/27 20:26:31 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/09/27 20:26:35 INFO mlflow.models.evaluation.default_evaluator: The evaluation dataset is inferred as binary dataset, positive label is 1, negative label is 0.\n",
      "2023/09/27 20:26:44 WARNING mlflow.models.evaluation.default_evaluator: Skip logging model explainability insights because the shap explainer None requires all feature values to be numeric, and each feature column must only contain scalar values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process credit card balance - done in 106s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance()\n",
    "                \n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        \n",
    "        # Nos colonnes bools sont transformées en object suite au merge. On rétabli la vérité\n",
    "        boolean_columns = [col for col in df.columns if pd.api.types.is_object_dtype(df[col])]\n",
    "        df[boolean_columns] = df[boolean_columns].astype(bool)\n",
    "        \n",
    "        del cc\n",
    "        gc.collect()\n",
    "    \n",
    "        lgbm_clf, valid_x, valid_y = train_lgbm(df)    \n",
    "\n",
    "        MlFlow_log_score(lgbm_clf, \"LGBM classifier\", \"lgbm: add credit card balance\", valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8359b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lgbm_clf, open('lgbm_client_scoring.pkl', 'wb'))\n",
    "df.to_csv('home_credit_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06a966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
